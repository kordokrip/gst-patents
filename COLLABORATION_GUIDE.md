# ğŸ¤ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ í˜‘ì—… ê°€ì´ë“œ

> ì„±í˜¸ë‹˜ì´ ì§ì ‘ ìˆ˜í–‰í•´ì•¼ í•  ì„œë²„ì‚¬ì´ë“œ ì‘ì—…ë“¤ì— ëŒ€í•œ ìƒì„¸í•œ ë‹¨ê³„ë³„ ì•ˆë‚´

## ğŸ¯ í˜‘ì—… ê°œìš”

í˜„ì¬ ì™„ì„±ëœ **ì •ì  ì›¹ì‚¬ì´íŠ¸ ê¸°ë°˜ íŠ¹í—ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ**ì„ **ì™„ì „í•œ RAG ì‹œìŠ¤í…œ**ìœ¼ë¡œ í™•ì¥í•˜ê¸° ìœ„í•´ì„œëŠ” ì„œë²„ì‚¬ì´ë“œ ê°œë°œì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ê°€ì´ë“œëŠ” ì„±í˜¸ë‹˜ê»˜ì„œ ì§ì ‘ ìˆ˜í–‰í•˜ì…”ì•¼ í•  ì‘ì—…ë“¤ì„ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ğŸ“‹ í˜‘ì—… ë¶„ë‹´

### ğŸŸ¢ ì´ë¯¸ ì™„ì„±ëœ ë¶€ë¶„ (ì •ì  ì›¹ì‚¬ì´íŠ¸)
- âœ… í”„ë¡ íŠ¸ì—”ë“œ UI/UX (HTML, CSS, JavaScript)
- âœ… ë°ì´í„° ì‹œê°í™” (Chart.js, ECharts)
- âœ… íŠ¹í—ˆ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë° ìƒ˜í”Œ ë°ì´í„°
- âœ… ë°˜ì‘í˜• ë””ìì¸ ë° ëª¨ë°”ì¼ ìµœì í™”
- âœ… ê²€ìƒ‰/í•„í„°ë§ ë¡œì§ (í”„ë¡ íŠ¸ì—”ë“œ)
- âœ… í”„ë¡œì íŠ¸ ë¬¸ì„œ ë° ì•„í‚¤í…ì²˜ ì„¤ê³„

### ğŸ”´ ì„±í˜¸ë‹˜ê»˜ì„œ êµ¬í˜„í•˜ì…”ì•¼ í•  ë¶€ë¶„ (ì„œë²„ì‚¬ì´ë“œ)
1. **ë°±ì—”ë“œ ì„œë²„ ê°œë°œ** (FastAPI, Streamlit)
2. **LLM API í†µí•©** (OpenAI, Claude, LLaMA)
3. **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•** (Chroma, Pinecone)
4. **PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸** (ë¬¸ì„œ ì „ì²˜ë¦¬)
5. **AI ëª¨ë¸ íŒŒì¸íŠœë‹** (QLoRA, SFT/DPO)
6. **ì¸ì¦ ë° ë³´ì•ˆ ì‹œìŠ¤í…œ**

---

## 1. ë°±ì—”ë“œ ì„œë²„ ê°œë°œ ê°€ì´ë“œ

### 1.1 FastAPI ì„œë²„ êµ¬ì¶•

#### Step 1: ê°œë°œ í™˜ê²½ ì¤€ë¹„

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìƒëµ)
cd gst-patent-management
python -m venv venv

# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install fastapi uvicorn python-multipart python-dotenv
```

#### Step 2: ê¸°ë³¸ FastAPI ì„œë²„ ìƒì„±

**íŒŒì¼: `backend/app/main.py`**
```python
from fastapi import FastAPI, HTTPException, UploadFile, File, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional
import os
import json
from datetime import datetime

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="GST Patent RAG API",
    description="ê¸€ë¡œë²Œ ìŠ¤íƒ ë‹¤ë“œ í…Œí¬ë†€ë¡œì§€ RAG ì‹œìŠ¤í…œ API",
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:8000"],  # í”„ë¡ íŠ¸ì—”ë“œ URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë³´ì•ˆ ì„¤ì •
security = HTTPBearer()

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class PatentQuery(BaseModel):
    query: str
    max_results: int = 5
    similarity_threshold: float = 0.7

class ChatMessage(BaseModel):
    message: str
    conversation_id: Optional[str] = None

class PatentCreate(BaseModel):
    title: str
    abstract: str
    patent_number: str
    category: str
    inventors: List[str]

# ê¸°ë³¸ ë¼ìš°íŠ¸
@app.get("/")
async def root():
    return {
        "service": "GST Patent RAG API",
        "version": "1.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }

# í—¬ìŠ¤ ì²´í¬
@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# ì¸ì¦ ì˜ì¡´ì„±
async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” JWT í† í° ê²€ì¦
    token = credentials.credentials
    if token != "your-secret-token":  # ì‹¤ì œë¡œëŠ” JWT ê²€ì¦
        raise HTTPException(status_code=401, detail="Invalid token")
    return {"user_id": "user123"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
```

#### Step 3: ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

```bash
# ì„œë²„ ì‹¤í–‰
cd backend
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
curl http://localhost:8000/
curl http://localhost:8000/health
```

### 1.2 Streamlit ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

#### Step 1: Streamlit ì•± ìƒì„±

**íŒŒì¼: `streamlit_app/chat_interface.py`**
```python
import streamlit as st
import requests
import json
from datetime import datetime
import uuid

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="GST Patent RAG Chat",
    page_icon="ğŸ”",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ” GST Patent RAG")
    st.write("ë°˜ë„ì²´ ìœ í•´ê°€ìŠ¤ ì •í™”ì¥ë¹„ íŠ¹í—ˆ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
    
    # API ì„¤ì •
    api_endpoint = st.text_input(
        "API Endpoint", 
        value="http://localhost:8000",
        help="FastAPI ì„œë²„ ì£¼ì†Œ"
    )
    
    api_key = st.text_input(
        "API Key", 
        type="password",
        help="ì¸ì¦ìš© API í‚¤"
    )
    
    if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
        st.session_state.messages = []
        st.session_state.conversation_id = str(uuid.uuid4())
        st.rerun()

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ’¬ íŠ¹í—ˆ ì§ˆì˜ì‘ë‹µ")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ (AI ì‘ë‹µì¸ ê²½ìš°)
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("ì°¸ì¡° íŠ¹í—ˆ ë¬¸ì„œ"):
                for source in message["sources"]:
                    st.write(f"**{source['patent_number']}**: {source['title']}")

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("íŠ¹í—ˆì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        try:
            # FastAPI ì„œë²„ë¡œ ìš”ì²­ ì „ì†¡
            response = requests.post(
                f"{api_endpoint}/api/v1/chat",
                json={
                    "message": prompt,
                    "conversation_id": st.session_state.conversation_id
                },
                headers={"Authorization": f"Bearer {api_key}"}
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result["response"]["message"]
                sources = result.get("sources", [])
                
                message_placeholder.markdown(answer)
                
                # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": answer,
                    "sources": sources
                })
                
                # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ
                if sources:
                    with st.expander("ì°¸ì¡° íŠ¹í—ˆ ë¬¸ì„œ"):
                        for source in sources:
                            st.write(f"**{source['patent_number']}**: {source['title']}")
                            st.write(f"ê´€ë ¨ë„: {source['relevance_score']:.2f}")
                            st.write("---")
            else:
                st.error(f"API ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
```

#### Step 2: Streamlit ì•± ì‹¤í–‰

```bash
# Streamlit ì„¤ì¹˜ (ì•„ì§ ì•ˆ í–ˆë‹¤ë©´)
pip install streamlit

# ì•± ì‹¤í–‰
cd streamlit_app
streamlit run chat_interface.py
```

---

## 2. LLM API í†µí•© ê°€ì´ë“œ

### 2.1 OpenAI ChatGPT API ì—°ë™

#### Step 1: API í‚¤ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > backend/.env << EOF
OPENAI_API_KEY=your-openai-api-key-here
ANTHROPIC_API_KEY=your-claude-api-key-here
UPSTAGE_API_KEY=your-upstage-api-key-here
EOF
```

#### Step 2: LLM ì„œë¹„ìŠ¤ êµ¬í˜„

**íŒŒì¼: `backend/app/services/llm_service.py`**
```python
import openai
import anthropic
from typing import List, Dict, Any, Optional
import os
from dotenv import load_dotenv
import logging

load_dotenv()
logger = logging.getLogger(__name__)

class LLMService:
    """
    ë‹¤ì¤‘ LLM ì œê³µìë¥¼ ì§€ì›í•˜ëŠ” ì„œë¹„ìŠ¤
    """
    
    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        self.initialize_clients()
    
    def initialize_clients(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸
            if os.getenv("OPENAI_API_KEY"):
                openai.api_key = os.getenv("OPENAI_API_KEY")
                self.openai_client = openai
                logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # Anthropic í´ë¼ì´ì–¸íŠ¸
            if os.getenv("ANTHROPIC_API_KEY"):
                self.anthropic_client = anthropic.Anthropic(
                    api_key=os.getenv("ANTHROPIC_API_KEY")
                )
                logger.info("âœ… Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def generate_response(
        self,
        prompt: str,
        context: str,
        model: str = "gpt-4",
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        """
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
            system_prompt = """
ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ìŠ¤íƒ ë‹¤ë“œ í…Œí¬ë†€ë¡œì§€ì˜ íŠ¹í—ˆ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë°˜ë„ì²´ ìœ í•´ê°€ìŠ¤ ì •í™”ì¥ë¹„ ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, 
ì œê³µëœ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
2. íŠ¹í—ˆ ë²ˆí˜¸ì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ ê·¼ê±° ì œì‹œ  
3. ê¸°ìˆ ì  ë‚´ìš©ì„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
4. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„
"""

            user_prompt = f"""
ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {prompt}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
            
            if model.startswith("gpt") and self.openai_client:
                response = await self._call_openai(
                    system_prompt, user_prompt, model, max_tokens, temperature
                )
            elif model.startswith("claude") and self.anthropic_client:
                response = await self._call_anthropic(
                    system_prompt, user_prompt, model, max_tokens, temperature
                )
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _call_openai(
        self, 
        system_prompt: str, 
        user_prompt: str, 
        model: str, 
        max_tokens: int, 
        temperature: float
    ) -> Dict[str, Any]:
        """OpenAI API í˜¸ì¶œ"""
        try:
            response = await self.openai_client.ChatCompletion.acreate(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=False
            )
            
            return {
                "content": response.choices[0].message.content,
                "model": model,
                "usage": response.usage._asdict(),
                "finish_reason": response.choices[0].finish_reason
            }
            
        except Exception as e:
            logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    async def _call_anthropic(
        self, 
        system_prompt: str, 
        user_prompt: str, 
        model: str, 
        max_tokens: int, 
        temperature: float
    ) -> Dict[str, Any]:
        """Anthropic API í˜¸ì¶œ"""
        try:
            response = await self.anthropic_client.messages.create(
                model="claude-3-haiku-20240307",  # ë˜ëŠ” ë‹¤ë¥¸ Claude ëª¨ë¸
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            return {
                "content": response.content[0].text,
                "model": model,
                "usage": response.usage._asdict(),
                "finish_reason": response.stop_reason
            }
            
        except Exception as e:
            logger.error(f"âŒ Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
llm_service = LLMService()
```

#### Step 3: FastAPI ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

**íŒŒì¼: `backend/app/api/chat.py`**
```python
from fastapi import APIRouter, HTTPException, Depends
from app.services.llm_service import llm_service
from app.services.rag_service import rag_service  # ë‹¤ìŒ ì„¹ì…˜ì—ì„œ êµ¬í˜„
from pydantic import BaseModel
from typing import Optional, List, Dict, Any

router = APIRouter()

class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None
    model: str = "gpt-4"
    max_tokens: int = 1000
    temperature: float = 0.7

class ChatResponse(BaseModel):
    response: Dict[str, Any]
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]

@router.post("/chat", response_model=ChatResponse)
async def chat_with_patents(
    request: ChatRequest,
    current_user=Depends(get_current_user)  # ì¸ì¦ í•„ìš”
):
    """
    íŠ¹í—ˆ ë¬¸ì„œ ê¸°ë°˜ RAG ì±„íŒ…
    """
    try:
        # 1. ê´€ë ¨ íŠ¹í—ˆ ë¬¸ì„œ ê²€ìƒ‰
        search_results = await rag_service.search_patents(
            query=request.message,
            k=5
        )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join([
            f"íŠ¹í—ˆë²ˆí˜¸: {doc['patent_number']}\n"
            f"ì œëª©: {doc['title']}\n"
            f"ë‚´ìš©: {doc['content']}\n"
            f"---"
            for doc in search_results
        ])
        
        # 3. LLM ì‘ë‹µ ìƒì„±
        llm_response = await llm_service.generate_response(
            prompt=request.message,
            context=context,
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        # 4. ì‘ë‹µ êµ¬ì„±
        return ChatResponse(
            response={
                "message": llm_response["content"],
                "conversation_id": request.conversation_id,
                "message_id": f"msg_{datetime.now().timestamp()}"
            },
            sources=[
                {
                    "patent_number": doc["patent_number"],
                    "title": doc["title"],
                    "relevance_score": doc.get("score", 0.0),
                    "chunk": doc["content"][:200] + "..."
                }
                for doc in search_results
            ],
            metadata={
                "model_used": request.model,
                "tokens_used": llm_response.get("usage", {}),
                "processing_time": 0.0,  # ì‹¤ì œ ì¸¡ì • í•„ìš”
                "confidence": 0.85
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ë©”ì¸ appì— ë¼ìš°í„° ì¶”ê°€
# backend/app/main.pyì— ì¶”ê°€:
# from app.api.chat import router as chat_router
# app.include_router(chat_router, prefix="/api/v1", tags=["chat"])
```

---

## 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ê°€ì´ë“œ

### 3.1 Chroma DB ì„¤ì •

#### Step 1: Chroma ì„¤ì¹˜ ë° ì„¤ì •

```bash
# Chroma ì„¤ì¹˜
pip install chromadb

# ì„ë² ë”© ëª¨ë¸ ì„¤ì¹˜
pip install sentence-transformers
```

#### Step 2: ë²¡í„° ì„œë¹„ìŠ¤ êµ¬í˜„

**íŒŒì¼: `backend/app/services/vector_service.py`**
```python
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import numpy as np
import logging
import os
from pathlib import Path

logger = logging.getLogger(__name__)

class VectorService:
    """
    Chroma ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤
    """
    
    def __init__(self):
        self.client = None
        self.collection = None
        self.embedding_model = None
        self.initialize()
    
    def initialize(self):
        """ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # Chroma í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            persist_directory = Path("./data/chroma_db")
            persist_directory.mkdir(parents=True, exist_ok=True)
            
            self.client = chromadb.PersistentClient(
                path=str(persist_directory),
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´ íŠ¹í™”)
            self.embedding_model = SentenceTransformer(
                'jhgan/ko-sroberta-multitask',  # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
                device='cpu'  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
            )
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
            try:
                self.collection = self.client.create_collection(
                    name="gst_patents",
                    metadata={"description": "GST íŠ¹í—ˆ ë¬¸ì„œ ë²¡í„° ì»¬ë ‰ì…˜"}
                )
                logger.info("âœ… ìƒˆ Chroma ì»¬ë ‰ì…˜ ìƒì„±")
            except Exception:
                self.collection = self.client.get_collection("gst_patents")
                logger.info("âœ… ê¸°ì¡´ Chroma ì»¬ë ‰ì…˜ ë¡œë“œ")
            
            logger.info("âœ… ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def add_documents(
        self, 
        documents: List[str], 
        metadatas: List[Dict[str, Any]], 
        ids: List[str]
    ):
        """ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€"""
        try:
            # ì„ë² ë”© ìƒì„±
            embeddings = self.embedding_model.encode(
                documents, 
                convert_to_numpy=True,
                show_progress_bar=True
            )
            
            # Chromaì— ì¶”ê°€
            self.collection.add(
                documents=documents,
                embeddings=embeddings.tolist(),
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë²¡í„° DB ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def search_similar_documents(
        self,
        query: str,
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_model.encode([query])
            
            # ë²¡í„° ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=n_results,
                where=where,
                include=['documents', 'metadatas', 'distances']
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for i in range(len(results['ids'][0])):
                formatted_results.append({
                    'id': results['ids'][0][i],
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'similarity_score': 1 - results['distances'][0][i],  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    'patent_number': results['metadatas'][0][i].get('patent_number'),
                    'title': results['metadatas'][0][i].get('title')
                })
            
            logger.info(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(formatted_results)}ê°œ ê²°ê³¼")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
    
    def update_document(self, id: str, document: str, metadata: Dict[str, Any]):
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        try:
            embedding = self.embedding_model.encode([document])
            
            self.collection.update(
                ids=[id],
                documents=[document],
                embeddings=embedding.tolist(),
                metadatas=[metadata]
            )
            
            logger.info(f"âœ… ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {id}")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    def delete_document(self, id: str):
        """ë¬¸ì„œ ì‚­ì œ"""
        try:
            self.collection.delete(ids=[id])
            logger.info(f"âœ… ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {id}")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            return {
                "total_documents": count,
                "collection_name": "gst_patents",
                "embedding_model": "jhgan/ko-sroberta-multitask",
                "status": "active"
            }
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": str(e)}

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
vector_service = VectorService()
```

### 3.2 íŠ¹í—ˆ ë¬¸ì„œ ë²¡í„°í™”

#### Step 1: ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

**íŒŒì¼: `backend/app/services/document_processor.py`**
```python
import PyPDF2
import re
from typing import List, Dict, Any, Tuple
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """
    íŠ¹í—ˆ ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ì„œë¹„ìŠ¤
    """
    
    def __init__(self):
        self.chunk_size = 1000
        self.chunk_overlap = 200
    
    def process_patent_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        íŠ¹í—ˆ PDF íŒŒì¼ ì²˜ë¦¬
        """
        try:
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = self.extract_text_from_pdf(pdf_path)
            
            # íŠ¹í—ˆ ì„¹ì…˜ ë¶„ë¦¬
            sections = self.parse_patent_sections(text)
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.create_chunks(text)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.extract_metadata(text)
            
            return {
                "full_text": text,
                "sections": sections,
                "chunks": chunks,
                "metadata": metadata,
                "total_chunks": len(chunks)
            }
            
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_path}: {e}")
            raise
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
                
                return text.strip()
                
        except Exception as e:
            logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    def parse_patent_sections(self, text: str) -> Dict[str, str]:
        """
        íŠ¹í—ˆ ë¬¸ì„œì˜ ì£¼ìš” ì„¹ì…˜ ë¶„ë¦¬
        """
        sections = {
            "abstract": "",
            "claims": "",
            "description": "",
            "background": ""
        }
        
        try:
            # ìš”ì•½ ì„¹ì…˜ ì¶”ì¶œ
            abstract_match = re.search(r'ìš”\s*ì•½\s*[:ï¼š]\s*(.*?)(?=ì²­êµ¬ë²”ìœ„|ë°œëª…ì˜\s*ìƒì„¸í•œ\s*ì„¤ëª…)', text, re.DOTALL)
            if abstract_match:
                sections["abstract"] = abstract_match.group(1).strip()
            
            # ì²­êµ¬ë²”ìœ„ ì„¹ì…˜ ì¶”ì¶œ
            claims_match = re.search(r'ì²­êµ¬ë²”ìœ„\s*[:ï¼š]\s*(.*?)(?=ë°œëª…ì˜\s*ìƒì„¸í•œ\s*ì„¤ëª…|ë„ë©´ì˜\s*ê°„ë‹¨í•œ\s*ì„¤ëª…)', text, re.DOTALL)
            if claims_match:
                sections["claims"] = claims_match.group(1).strip()
            
            # ë°œëª…ì˜ ìƒì„¸í•œ ì„¤ëª… ì¶”ì¶œ
            description_match = re.search(r'ë°œëª…ì˜\s*ìƒì„¸í•œ\s*ì„¤ëª…\s*[:ï¼š]\s*(.*?)(?=ì²­êµ¬ë²”ìœ„|ë„ë©´ì˜\s*ê°„ë‹¨í•œ\s*ì„¤ëª…)', text, re.DOTALL)
            if description_match:
                sections["description"] = description_match.group(1).strip()
            
            return sections
            
        except Exception as e:
            logger.error(f"âŒ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return sections
    
    def create_chunks(self, text: str) -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
        """
        try:
            # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
            
            chunks = []
            current_chunk = ""
            chunk_id = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # ì²­í¬ í¬ê¸° í™•ì¸
                if len(current_chunk + sentence) > self.chunk_size:
                    if current_chunk:
                        chunks.append({
                            "chunk_id": chunk_id,
                            "content": current_chunk.strip(),
                            "start_pos": len(text) - len(' '.join(sentences[chunk_id:])),
                            "length": len(current_chunk)
                        })
                        chunk_id += 1
                    
                    # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ë³´ì¡´
                    overlap_text = ' '.join(current_chunk.split()[-self.chunk_overlap//10:])
                    current_chunk = overlap_text + " " + sentence
                else:
                    current_chunk += " " + sentence
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk.strip():
                chunks.append({
                    "chunk_id": chunk_id,
                    "content": current_chunk.strip(),
                    "start_pos": len(text) - len(current_chunk),
                    "length": len(current_chunk)
                })
            
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            raise
    
    def extract_metadata(self, text: str) -> Dict[str, Any]:
        """
        íŠ¹í—ˆ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        """
        metadata = {}
        
        try:
            # íŠ¹í—ˆë²ˆí˜¸ ì¶”ì¶œ
            patent_num_match = re.search(r'íŠ¹í—ˆë²ˆí˜¸\s*[:ï¼š]?\s*(\d+-\d+)', text)
            if patent_num_match:
                metadata["patent_number"] = patent_num_match.group(1)
            
            # ë°œëª…ì˜ ëª…ì¹­ ì¶”ì¶œ
            title_match = re.search(r'ë°œëª…ì˜\s*ëª…ì¹­\s*[:ï¼š]?\s*(.*?)(?:\n|$)', text)
            if title_match:
                metadata["title"] = title_match.group(1).strip()
            
            # ë°œëª…ì ì¶”ì¶œ
            inventor_match = re.search(r'ë°œëª…ì\s*[:ï¼š]?\s*(.*?)(?:\n|ì¶œì›ì¸)', text, re.DOTALL)
            if inventor_match:
                inventors = [name.strip() for name in inventor_match.group(1).split(',')]
                metadata["inventors"] = inventors
            
            # ë“±ë¡ì¼ ì¶”ì¶œ  
            reg_date_match = re.search(r'ë“±ë¡ì¼\s*[:ï¼š]?\s*(\d{4}[-./]\d{2}[-./]\d{2})', text)
            if reg_date_match:
                metadata["registration_date"] = reg_date_match.group(1)
            
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return metadata

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
document_processor = DocumentProcessor()
```

#### Step 2: ë²¡í„°í™” API ì—”ë“œí¬ì¸íŠ¸

**íŒŒì¼: `backend/app/api/vector.py`**
```python
from fastapi import APIRouter, HTTPException, UploadFile, File, Depends
from app.services.vector_service import vector_service
from app.services.document_processor import document_processor
from typing import List, Dict, Any
import tempfile
import os
from pathlib import Path

router = APIRouter()

@router.post("/upload-patents")
async def upload_patent_documents(
    files: List[UploadFile] = File(...),
    current_user=Depends(get_current_user)
):
    """
    íŠ¹í—ˆ PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ê³  ë²¡í„°í™”
    """
    results = []
    
    for file in files:
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                content = await file.read()
                tmp_file.write(content)
                tmp_file_path = tmp_file.name
            
            # ë¬¸ì„œ ì²˜ë¦¬
            processed_doc = document_processor.process_patent_pdf(tmp_file_path)
            
            # ì²­í¬ë³„ë¡œ ë²¡í„° DBì— ì¶”ê°€
            documents = []
            metadatas = []
            ids = []
            
            patent_number = processed_doc["metadata"].get("patent_number", file.filename)
            
            for i, chunk in enumerate(processed_doc["chunks"]):
                chunk_id = f"{patent_number}_chunk_{i}"
                
                documents.append(chunk["content"])
                metadatas.append({
                    **processed_doc["metadata"],
                    "chunk_id": chunk["chunk_id"],
                    "chunk_type": "content",
                    "source_file": file.filename
                })
                ids.append(chunk_id)
            
            # ë²¡í„° DBì— ì¶”ê°€
            vector_service.add_documents(documents, metadatas, ids)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
            
            results.append({
                "filename": file.filename,
                "patent_number": patent_number,
                "chunks_created": len(documents),
                "status": "success"
            })
            
        except Exception as e:
            results.append({
                "filename": file.filename,
                "status": "error",
                "error": str(e)
            })
    
    return {"results": results}

@router.get("/search")
async def search_patents(
    query: str,
    limit: int = 5,
    similarity_threshold: float = 0.7,
    current_user=Depends(get_current_user)
):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ íŠ¹í—ˆ ê²€ìƒ‰
    """
    try:
        results = vector_service.search_similar_documents(
            query=query,
            n_results=limit
        )
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
        filtered_results = [
            result for result in results 
            if result["similarity_score"] >= similarity_threshold
        ]
        
        return {
            "query": query,
            "results": filtered_results,
            "total_found": len(filtered_results),
            "search_time": 0.0  # ì‹¤ì œ ì¸¡ì • í•„ìš”
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/collection-info")
async def get_collection_info(current_user=Depends(get_current_user)):
    """
    ë²¡í„° DB ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ
    """
    return vector_service.get_collection_info()

# ë©”ì¸ ì•±ì— ë¼ìš°í„° ì¶”ê°€
# backend/app/main.pyì— ì¶”ê°€:
# from app.api.vector import router as vector_router  
# app.include_router(vector_router, prefix="/api/v1/vector", tags=["vector"])
```

---

## 4. PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### 4.1 ê³ ê¸‰ PDF ì²˜ë¦¬

#### Step 1: ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install PyPDF2 pdfplumber pdfminer.six

# í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬
pip install konlpy soynlp

# ì´ë¯¸ì§€ ì²˜ë¦¬ (OCR)
pip install pytesseract pillow

# í…Œì´ë¸” ì¶”ì¶œ
pip install tabula-py camelot-py
```

#### Step 2: ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ê¸°

**íŒŒì¼: `backend/app/services/advanced_processor.py`**
```python
import pdfplumber
import pytesseract
from PIL import Image
import re
from typing import List, Dict, Any, Optional
import logging
from konlpy.tag import Okt
import json

logger = logging.getLogger(__name__)

class AdvancedDocumentProcessor:
    """
    ê³ ê¸‰ íŠ¹í—ˆ ë¬¸ì„œ ì²˜ë¦¬ê¸°
    """
    
    def __init__(self):
        self.okt = Okt()  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°
        self.patent_sections = [
            "ìš”ì•½", "ì²­êµ¬ë²”ìœ„", "ë°œëª…ì˜ ìƒì„¸í•œ ì„¤ëª…", 
            "ë„ë©´ì˜ ê°„ë‹¨í•œ ì„¤ëª…", "ë°œëª…ì˜ ë°°ê²½"
        ]
    
    def extract_comprehensive_data(self, pdf_path: str) -> Dict[str, Any]:
        """
        í¬ê´„ì ì¸ íŠ¹í—ˆ ë°ì´í„° ì¶”ì¶œ
        """
        try:
            with pdfplumber.open(pdf_path) as pdf:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                full_text = self.extract_text_with_structure(pdf)
                
                # í…Œì´ë¸” ì¶”ì¶œ
                tables = self.extract_tables(pdf)
                
                # ì´ë¯¸ì§€ ë° ë„ë©´ ì •ë³´
                images = self.extract_image_info(pdf)
                
                # êµ¬ì¡°í™”ëœ ì„¹ì…˜ íŒŒì‹±
                structured_sections = self.parse_structured_sections(full_text)
                
                # ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ
                technical_keywords = self.extract_technical_keywords(full_text)
                
                # ì²­êµ¬í•­ ë¶„ì„
                claims_analysis = self.analyze_claims(structured_sections.get("ì²­êµ¬ë²”ìœ„", ""))
                
                return {
                    "full_text": full_text,
                    "structured_sections": structured_sections,
                    "tables": tables,
                    "images": images,
                    "technical_keywords": technical_keywords,
                    "claims_analysis": claims_analysis,
                    "metadata": self.extract_enhanced_metadata(full_text)
                }
                
        except Exception as e:
            logger.error(f"âŒ í¬ê´„ì  ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    def extract_text_with_structure(self, pdf) -> str:
        """êµ¬ì¡° ì •ë³´ë¥¼ ìœ ì§€í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        full_text = ""
        
        for page_num, page in enumerate(pdf.pages):
            # í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.extract_text()
            if text:
                full_text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n{text}\n"
        
        return full_text
    
    def extract_tables(self, pdf) -> List[Dict[str, Any]]:
        """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
        tables = []
        
        for page_num, page in enumerate(pdf.pages):
            page_tables = page.extract_tables()
            
            for table_num, table in enumerate(page_tables):
                if table:
                    tables.append({
                        "page": page_num + 1,
                        "table_id": f"table_{page_num}_{table_num}",
                        "data": table,
                        "rows": len(table),
                        "cols": len(table[0]) if table else 0
                    })
        
        return tables
    
    def extract_image_info(self, pdf) -> List[Dict[str, Any]]:
        """ì´ë¯¸ì§€ ë° ë„ë©´ ì •ë³´ ì¶”ì¶œ"""
        images = []
        
        for page_num, page in enumerate(pdf.pages):
            if hasattr(page, 'images'):
                for img_num, image in enumerate(page.images):
                    images.append({
                        "page": page_num + 1,
                        "image_id": f"img_{page_num}_{img_num}",
                        "bbox": image.get('bbox', []),
                        "width": image.get('width', 0),
                        "height": image.get('height', 0)
                    })
        
        return images
    
    def parse_structured_sections(self, text: str) -> Dict[str, str]:
        """êµ¬ì¡°í™”ëœ ì„¹ì…˜ íŒŒì‹±"""
        sections = {}
        
        for section_name in self.patent_sections:
            # ê° ì„¹ì…˜ë³„ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
            patterns = [
                f"{section_name}\\s*[:ï¼š]\\s*(.*?)(?={'|'.join(self.patent_sections)}|$)",
                f"{section_name}\\s*(.*?)(?={'|'.join(self.patent_sections)}|$)"
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                if match:
                    sections[section_name] = match.group(1).strip()
                    break
        
        return sections
    
    def extract_technical_keywords(self, text: str) -> List[Dict[str, Any]]:
        """ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„"""
        try:
            # ëª…ì‚¬ ì¶”ì¶œ
            nouns = self.okt.nouns(text)
            
            # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
            tech_keywords = [
                noun for noun in nouns 
                if len(noun) >= 2 and self.is_technical_term(noun)
            ]
            
            # ë¹ˆë„ ê³„ì‚°
            from collections import Counter
            keyword_counts = Counter(tech_keywords)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
            top_keywords = [
                {
                    "keyword": keyword,
                    "frequency": count,
                    "category": self.classify_keyword(keyword)
                }
                for keyword, count in keyword_counts.most_common(50)
            ]
            
            return top_keywords
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def is_technical_term(self, term: str) -> bool:
        """ê¸°ìˆ  ìš©ì–´ ì—¬ë¶€ íŒë‹¨"""
        technical_indicators = [
            "ì‹œìŠ¤í…œ", "ì¥ì¹˜", "ë°©ë²•", "ê³µì •", "ê¸°ìˆ ", "ì œì–´", 
            "ì²˜ë¦¬", "ë¶„ì„", "ì¸¡ì •", "ì„¼ì„œ", "ëª¨ë‹ˆí„°", "ì•Œê³ ë¦¬ì¦˜",
            "ìŠ¤í¬ëŸ¬ë²„", "ì¹ ëŸ¬", "í”Œë¼ì¦ˆë§ˆ", "ì˜¨ë„", "ê°€ìŠ¤", "ì •í™”",
            "ë°˜ë„ì²´", "ì œì¡°", "ì„¤ë¹„", "íš¨ìœ¨", "ìµœì í™”"
        ]
        
        return any(indicator in term for indicator in technical_indicators)
    
    def classify_keyword(self, keyword: str) -> str:
        """í‚¤ì›Œë“œ ë¶„ë¥˜"""
        categories = {
            "ì¥ë¹„": ["ìŠ¤í¬ëŸ¬ë²„", "ì¹ ëŸ¬", "ì¥ì¹˜", "ì„¤ë¹„", "ì‹œìŠ¤í…œ"],
            "ê³µì •": ["ì œì¡°", "ê³µì •", "ì²˜ë¦¬", "ê°€ê³µ", "ìƒì‚°"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "ë°©ë²•", "ì•Œê³ ë¦¬ì¦˜", "ì œì–´", "ìµœì í™”"],
            "ë¬¼ì§ˆ": ["ê°€ìŠ¤", "í™”í•™", "ë¬¼ì§ˆ", "ì¬ë£Œ"],
            "ì¸¡ì •": ["ì„¼ì„œ", "ì¸¡ì •", "ëª¨ë‹ˆí„°", "ë¶„ì„", "ê²€ì‚¬"]
        }
        
        for category, terms in categories.items():
            if any(term in keyword for term in terms):
                return category
        
        return "ê¸°íƒ€"
    
    def analyze_claims(self, claims_text: str) -> Dict[str, Any]:
        """ì²­êµ¬í•­ ë¶„ì„"""
        if not claims_text:
            return {}
        
        try:
            # ë…ë¦½ ì²­êµ¬í•­ê³¼ ì¢…ì† ì²­êµ¬í•­ ë¶„ë¦¬
            claim_pattern = r'ì²­êµ¬í•­\s*(\d+)\s*[.ï¼]\s*(.*?)(?=ì²­êµ¬í•­\s*\d+|$)'
            claims = re.findall(claim_pattern, claims_text, re.DOTALL)
            
            independent_claims = []
            dependent_claims = []
            
            for claim_num, claim_text in claims:
                claim_text = claim_text.strip()
                
                # ì¢…ì† ì²­êµ¬í•­ ì—¬ë¶€ í™•ì¸
                if re.search(r'ì²­êµ¬í•­\s*\d+ì—\s*ìˆì–´ì„œ|ì²­êµ¬í•­\s*\d+ì—\s*ì˜í•˜ë©´', claim_text):
                    dependent_claims.append({
                        "number": int(claim_num),
                        "text": claim_text,
                        "parent_claim": self.find_parent_claim(claim_text)
                    })
                else:
                    independent_claims.append({
                        "number": int(claim_num),
                        "text": claim_text,
                        "components": self.extract_claim_components(claim_text)
                    })
            
            return {
                "total_claims": len(claims),
                "independent_claims": independent_claims,
                "dependent_claims": dependent_claims,
                "claim_structure": self.analyze_claim_structure(claims)
            }
            
        except Exception as e:
            logger.error(f"âŒ ì²­êµ¬í•­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def find_parent_claim(self, claim_text: str) -> Optional[int]:
        """ì¢…ì† ì²­êµ¬í•­ì˜ ë¶€ëª¨ ì²­êµ¬í•­ ì°¾ê¸°"""
        match = re.search(r'ì²­êµ¬í•­\s*(\d+)', claim_text)
        return int(match.group(1)) if match else None
    
    def extract_claim_components(self, claim_text: str) -> List[str]:
        """ì²­êµ¬í•­ êµ¬ì„±ìš”ì†Œ ì¶”ì¶œ"""
        # ì„¸ë¯¸ì½œë¡ ì´ë‚˜ ë²ˆí˜¸ë¡œ êµ¬ë¶„ëœ êµ¬ì„±ìš”ì†Œë“¤ ì¶”ì¶œ
        components = re.split(r'[;ï¼›]\s*(?=\d+\.|\w+\.)', claim_text)
        return [comp.strip() for comp in components if comp.strip()]
    
    def analyze_claim_structure(self, claims: List[tuple]) -> Dict[str, Any]:
        """ì²­êµ¬í•­ êµ¬ì¡° ë¶„ì„"""
        return {
            "claim_dependency_tree": self.build_dependency_tree(claims),
            "avg_claim_length": sum(len(claim[1]) for claim in claims) / len(claims) if claims else 0,
            "technical_complexity": self.assess_technical_complexity(claims)
        }
    
    def build_dependency_tree(self, claims: List[tuple]) -> Dict[str, List[int]]:
        """ì²­êµ¬í•­ ì˜ì¡´ì„± íŠ¸ë¦¬ êµ¬ì¶•"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì˜ì¡´ì„± ë¶„ì„ í•„ìš”
        return {}
    
    def assess_technical_complexity(self, claims: List[tuple]) -> str:
        """ê¸°ìˆ ì  ë³µì¡ë„ í‰ê°€"""
        total_length = sum(len(claim[1]) for claim in claims)
        avg_length = total_length / len(claims) if claims else 0
        
        if avg_length > 500:
            return "high"
        elif avg_length > 200:
            return "medium"
        else:
            return "low"
    
    def extract_enhanced_metadata(self, text: str) -> Dict[str, Any]:
        """í–¥ìƒëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        patterns = {
            "patent_number": r"íŠ¹í—ˆë²ˆí˜¸\s*[:ï¼š]?\s*(\d+-\d+)",
            "application_number": r"ì¶œì›ë²ˆí˜¸\s*[:ï¼š]?\s*(\d+-\d+)", 
            "title": r"ë°œëª…ì˜\s*ëª…ì¹­\s*[:ï¼š]?\s*(.*?)(?:\n|$)",
            "applicant": r"ì¶œì›ì¸\s*[:ï¼š]?\s*(.*?)(?:\n|ë°œëª…ì)",
            "inventor": r"ë°œëª…ì\s*[:ï¼š]?\s*(.*?)(?:\n|ì¶œì›ì¸)",
            "registration_date": r"ë“±ë¡ì¼\s*[:ï¼š]?\s*(\d{4}[-./]\d{2}[-./]\d{2})",
            "application_date": r"ì¶œì›ì¼\s*[:ï¼š]?\s*(\d{4}[-./]\d{2}[-./]\d{2})",
            "ipc_classification": r"êµ­ì œíŠ¹í—ˆë¶„ë¥˜\s*[:ï¼š]?\s*([A-H]\d{2}[A-Z]\s*\d+/\d+)"
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                
                # ë°œëª…ìë‚˜ ì¶œì›ì¸ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if key in ["inventor", "applicant"]:
                    value = [name.strip() for name in value.split(',')]
                
                metadata[key] = value
        
        return metadata

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
advanced_processor = AdvancedDocumentProcessor()
```

---

## 5. AI ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œ

### 5.1 QLoRA íŒŒì¸íŠœë‹ í™˜ê²½ ì„¤ì •

#### Step 1: GPU í™˜ê²½ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# GPU í™•ì¸ (CUDA í•„ìš”)
nvidia-smi

# PyTorch GPU ë²„ì „ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# íŒŒì¸íŠœë‹ ê´€ë ¨ íŒ¨í‚¤ì§€
pip install transformers==4.35.0
pip install peft==0.6.0
pip install bitsandbytes==0.41.1
pip install accelerate==0.24.1
pip install datasets==2.14.6
pip install trl==0.7.4

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
pip install wandb  # ì‹¤í—˜ ì¶”ì 
pip install tensorboard  # ëª¨ë‹ˆí„°ë§
```

#### Step 2: íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„

**íŒŒì¼: `backend/training/data_preparation.py`**
```python
import json
from typing import List, Dict, Any
from datasets import Dataset
import pandas as pd
from pathlib import Path

class PatentTrainingDataProcessor:
    """
    íŠ¹í—ˆ ë°ì´í„°ë¥¼ LLM íŒŒì¸íŠœë‹ìš©ìœ¼ë¡œ ë³€í™˜
    """
    
    def __init__(self):
        self.instruction_template = """ë‹¤ìŒì€ íŠ¹í—ˆ ë¬¸ì„œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ê³¼ ë‹µë³€ì…ë‹ˆë‹¤.

### íŠ¹í—ˆ ì •ë³´:
{patent_info}

### ì§ˆë¬¸:
{question}

### ë‹µë³€:
{answer}"""
    
    def create_training_dataset(
        self, 
        patents_data: List[Dict[str, Any]], 
        output_path: str = "training_data.json"
    ) -> Dataset:
        """
        íŠ¹í—ˆ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
        """
        training_examples = []
        
        for patent in patents_data:
            # ê° íŠ¹í—ˆì— ëŒ€í•´ ë‹¤ì–‘í•œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±
            examples = self.generate_qa_pairs(patent)
            training_examples.extend(examples)
        
        # Alpaca í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        alpaca_format = []
        for example in training_examples:
            alpaca_format.append({
                "instruction": example["instruction"],
                "input": example["input"],
                "output": example["output"]
            })
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(alpaca_format, f, ensure_ascii=False, indent=2)
        
        # Dataset ê°ì²´ë¡œ ë³€í™˜
        dataset = Dataset.from_pandas(pd.DataFrame(alpaca_format))
        return dataset
    
    def generate_qa_pairs(self, patent: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ë‹¨ì¼ íŠ¹í—ˆë¡œë¶€í„° ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±
        """
        qa_pairs = []
        
        # ê¸°ë³¸ ì •ë³´ ì§ˆë¬¸ë“¤
        basic_questions = [
            {
                "question": f"íŠ¹í—ˆë²ˆí˜¸ {patent['patent_number']}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "answer": f"íŠ¹í—ˆë²ˆí˜¸ {patent['patent_number']}ì€ '{patent['title']}'ë¼ëŠ” ë°œëª…ì— ê´€í•œ íŠ¹í—ˆì…ë‹ˆë‹¤. {patent['abstract']}"
            },
            {
                "question": f"ì´ íŠ¹í—ˆì˜ ì£¼ìš” ê¸°ìˆ  ë¶„ì•¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": f"ì´ íŠ¹í—ˆëŠ” {patent['technology_field']} ë¶„ì•¼ì˜ ê¸°ìˆ ë¡œ, {patent['category']} ì¹´í…Œê³ ë¦¬ì— ì†í•©ë‹ˆë‹¤."
            },
            {
                "question": f"ë°œëª…ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
                "answer": f"ì´ íŠ¹í—ˆì˜ ë°œëª…ìëŠ” {', '.join(patent.get('inventors', []))}ì…ë‹ˆë‹¤."
            }
        ]
        
        # ê¸°ìˆ ì  ì§ˆë¬¸ë“¤ 
        if patent.get('main_claims'):
            basic_questions.append({
                "question": "ì´ íŠ¹í—ˆì˜ ì£¼ìš” ì²­êµ¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": patent['main_claims']
            })
        
        if patent.get('technical_keywords'):
            basic_questions.append({
                "question": "ì´ íŠ¹í—ˆì™€ ê´€ë ¨ëœ ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": f"ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œëŠ” {', '.join(patent['technical_keywords'])}ì…ë‹ˆë‹¤."
            })
        
        # ê´€ë ¨ íŠ¹í—ˆ ì§ˆë¬¸
        if patent.get('related_patents'):
            basic_questions.append({
                "question": "ê´€ë ¨ëœ ë‹¤ë¥¸ íŠ¹í—ˆê°€ ìˆë‚˜ìš”?",
                "answer": f"ê´€ë ¨ íŠ¹í—ˆë¡œëŠ” {', '.join(patent['related_patents'])}ì´ ìˆìŠµë‹ˆë‹¤."
            })
        
        # Alpaca í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for qa in basic_questions:
            patent_info = self.format_patent_info(patent)
            
            qa_pairs.append({
                "instruction": "íŠ¹í—ˆ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.",
                "input": f"íŠ¹í—ˆ ì •ë³´: {patent_info}\n\nì§ˆë¬¸: {qa['question']}",
                "output": qa['answer']
            })
        
        return qa_pairs
    
    def format_patent_info(self, patent: Dict[str, Any]) -> str:
        """íŠ¹í—ˆ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§¤íŒ…"""
        info = f"íŠ¹í—ˆë²ˆí˜¸: {patent['patent_number']}\n"
        info += f"ë°œëª…ëª…ì¹­: {patent['title']}\n"
        info += f"ê¸°ìˆ ë¶„ì•¼: {patent['technology_field']}\n"
        info += f"ìš”ì•½: {patent['abstract']}"
        
        if patent.get('inventors'):
            info += f"\në°œëª…ì: {', '.join(patent['inventors'])}"
        
        return info

# ì‚¬ìš© ì˜ˆì‹œ
def prepare_training_data():
    """ì„±í˜¸ë‹˜ì´ ì‹¤í–‰í•  ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜"""
    
    # 1. ê¸°ì¡´ íŠ¹í—ˆ ë°ì´í„° ë¡œë“œ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ë˜ ë°ì´í„°)
    # ì´ ë¶€ë¶„ì€ ì‹¤ì œ íŠ¹í—ˆ ë°ì´í„°ë¡œ êµì²´
    patents_data = [
        # ì—¬ê¸°ì— ì‹¤ì œ 78ê°œ íŠ¹í—ˆ ë°ì´í„° ì…ë ¥
        # ë˜ëŠ” API/íŒŒì¼ì—ì„œ ë¡œë“œ
    ]
    
    # 2. í›ˆë ¨ ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = PatentTrainingDataProcessor()
    
    # 3. í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
    dataset = processor.create_training_dataset(patents_data)
    
    # 4. í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
    train_test_split = dataset.train_test_split(test_size=0.1)
    train_dataset = train_test_split['train']
    eval_dataset = train_test_split['test']
    
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}ê°œ")
    
    return train_dataset, eval_dataset

if __name__ == "__main__":
    train_dataset, eval_dataset = prepare_training_data()
```

#### Step 3: QLoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼: `backend/training/finetune_llama.py`**
```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
import wandb
from datetime import datetime
import os

class PatentLLMTrainer:
    """
    íŠ¹í—ˆ ë„ë©”ì¸ LLM íŒŒì¸íŠœë‹ íŠ¸ë ˆì´ë„ˆ
    """
    
    def __init__(self, model_name: str = "meta-llama/Llama-2-7b-chat-hf"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
    def setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        
        # BitsAndBytesConfig for 4-bit quantization
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            token="your-huggingface-token"  # Llama ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ í† í°
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ  
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            token="your-huggingface-token"
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        
        # ëª¨ë¸ì„ k-bit í›ˆë ¨ìš©ìœ¼ë¡œ ì¤€ë¹„
        self.model = prepare_model_for_kbit_training(self.model)
        
        print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    def setup_lora_config(self):
        """LoRA ì„¤ì •"""
        peft_config = LoraConfig(
            r=16,  # LoRA rank
            lora_alpha=32,  # LoRA scaling parameter
            target_modules=[
                "q_proj",
                "k_proj", 
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
                "lm_head",
            ],
            bias="none",
            lora_dropout=0.05,
            task_type="CAUSAL_LM",
        )
        
        # LoRA ì–´ëŒ‘í„° ì ìš©
        self.model = get_peft_model(self.model, peft_config)
        self.model.print_trainable_parameters()
        
        print("âœ… LoRA ì„¤ì • ì™„ë£Œ")
    
    def train(
        self, 
        train_dataset, 
        eval_dataset=None,
        output_dir: str = "./results",
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        max_seq_length: int = 1024
    ):
        """ëª¨ë¸ í›ˆë ¨"""
        
        # Weights & Biases ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
        wandb.init(
            project="gst-patent-llm",
            name=f"llama-finetune-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            config={
                "model_name": self.model_name,
                "num_epochs": num_train_epochs,
                "batch_size": per_device_train_batch_size,
                "learning_rate": learning_rate,
                "max_seq_length": max_seq_length
            }
        )
        
        # í›ˆë ¨ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            optim="adamw_torch",
            save_steps=500,
            logging_steps=100,
            learning_rate=learning_rate,
            weight_decay=0.001,
            fp16=False,
            bf16=True,
            max_grad_norm=0.3,
            max_steps=-1,
            warmup_ratio=0.03,
            group_by_length=True,
            lr_scheduler_type="cosine",
            report_to="wandb",
            save_strategy="steps",
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=500 if eval_dataset else None,
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="eval_loss" if eval_dataset else None,
        )
        
        # SFT íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        self.trainer = SFTTrainer(
            model=self.model,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            peft_config=None,  # ì´ë¯¸ ëª¨ë¸ì— ì ìš©ë¨
            dataset_text_field="text",  # ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ í•„ë“œëª…
            max_seq_length=max_seq_length,
            tokenizer=self.tokenizer,
            args=training_args,
            packing=False,
        )
        
        print("ğŸš€ í›ˆë ¨ ì‹œì‘...")
        
        # í›ˆë ¨ ì‹¤í–‰
        self.trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        self.trainer.save_model()
        
        print("âœ… í›ˆë ¨ ì™„ë£Œ ë° ëª¨ë¸ ì €ì¥")
    
    def format_training_data(self, dataset):
        """í›ˆë ¨ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        def format_instruction(example):
            # Alpaca í…œí”Œë¦¿ ì‚¬ìš©
            if example.get("input"):
                text = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
            else:
                text = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{example['instruction']}

### Response:
{example['output']}"""
            
            return {"text": text}
        
        return dataset.map(format_instruction)

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
def run_finetuning():
    """ì„±í˜¸ë‹˜ì´ ì‹¤í–‰í•  íŒŒì¸íŠœë‹ í•¨ìˆ˜"""
    
    # 1. í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ë°ì´í„°)
    from data_preparation import prepare_training_data
    train_dataset, eval_dataset = prepare_training_data()
    
    # 2. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = PatentLLMTrainer()
    
    # 3. ëª¨ë¸ ì„¤ì •
    trainer.setup_model_and_tokenizer()
    trainer.setup_lora_config()
    
    # 4. ë°ì´í„° í˜•ì‹ ë³€í™˜
    train_dataset = trainer.format_training_data(train_dataset)
    eval_dataset = trainer.format_training_data(eval_dataset)
    
    # 5. í›ˆë ¨ ì‹¤í–‰
    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        output_dir="./gst_patent_model",
        num_train_epochs=3,
        per_device_train_batch_size=2,  # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
        gradient_accumulation_steps=8,
        learning_rate=2e-4
    )
    
    print("ğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")

if __name__ == "__main__":
    run_finetuning()
```

### 5.2 í›ˆë ¨ ì‹¤í–‰ ê°€ì´ë“œ

#### Step 1: GPU í™˜ê²½ í™•ì¸

```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# CUDA ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
python -c "import torch; print(torch.cuda.get_device_name(0))"
```

#### Step 2: í›ˆë ¨ ì‹¤í–‰

```bash
# í›ˆë ¨ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd backend/training

# í›ˆë ¨ ì‹¤í–‰ (í™”ë©´ ì„¸ì…˜ ì‚¬ìš© ê¶Œì¥)
screen -S patent_training
python finetune_llama.py

# í™”ë©´ ì„¸ì…˜ì—ì„œ ë‚˜ê°€ê¸° (Ctrl+A, D)
# ë‹¤ì‹œ ì ‘ì†: screen -r patent_training
```

---

## 6. ì¸ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•

### 6.1 JWT ê¸°ë°˜ ì¸ì¦

#### Step 1: ì¸ì¦ ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install python-jose[cryptography]
pip install passlib[bcrypt]
pip install python-multipart
```

#### Step 2: ì¸ì¦ ì„œë¹„ìŠ¤ êµ¬í˜„

**íŒŒì¼: `backend/app/services/auth_service.py`**
```python
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from jose import JWTError, jwt
from passlib.context import CryptContext
from pydantic import BaseModel
import os
from dotenv import load_dotenv

load_dotenv()

# ë³´ì•ˆ ì„¤ì •
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key-change-in-production")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

# ë¹„ë°€ë²ˆí˜¸ í•´ì‹±
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

class Token(BaseModel):
    access_token: str
    token_type: str
    expires_in: int

class UserInDB(BaseModel):
    username: str
    email: str
    hashed_password: str
    is_active: bool = True
    role: str = "user"

class AuthService:
    """ì¸ì¦ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë¦¬
        self.users_db = {
            "admin": {
                "username": "admin",
                "email": "admin@gst.com", 
                "hashed_password": self.get_password_hash("gst_admin_2024"),
                "is_active": True,
                "role": "admin"
            },
            "user": {
                "username": "user",
                "email": "user@gst.com",
                "hashed_password": self.get_password_hash("gst_user_2024"),
                "is_active": True,
                "role": "user"
            }
        }
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        return pwd_context.verify(plain_password, hashed_password)
    
    def get_password_hash(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹±"""
        return pwd_context.hash(password)
    
    def authenticate_user(self, username: str, password: str) -> Optional[UserInDB]:
        """ì‚¬ìš©ì ì¸ì¦"""
        user_data = self.users_db.get(username)
        if not user_data:
            return None
        
        user = UserInDB(**user_data)
        if not self.verify_password(password, user.hashed_password):
            return None
        
        return user
    
    def create_access_token(
        self, 
        data: Dict[str, Any], 
        expires_delta: Optional[timedelta] = None
    ) -> str:
        """ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
        to_encode = data.copy()
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
        
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
        
        return encoded_jwt
    
    def verify_token(self, token: str) -> Optional[Dict[str, Any]]:
        """í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
            username: str = payload.get("sub")
            
            if username is None:
                return None
            
            # ì‚¬ìš©ì ì¡´ì¬ í™•ì¸
            user_data = self.users_db.get(username)
            if user_data is None:
                return None
            
            return {
                "username": username,
                "role": user_data["role"],
                "email": user_data["email"]
            }
            
        except JWTError:
            return None
    
    def create_user(
        self, 
        username: str, 
        email: str, 
        password: str, 
        role: str = "user"
    ) -> UserInDB:
        """ìƒˆ ì‚¬ìš©ì ìƒì„±"""
        if username in self.users_db:
            raise ValueError("ì‚¬ìš©ìëª…ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
        
        hashed_password = self.get_password_hash(password)
        user_data = {
            "username": username,
            "email": email,
            "hashed_password": hashed_password,
            "is_active": True,
            "role": role
        }
        
        self.users_db[username] = user_data
        return UserInDB(**user_data)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
auth_service = AuthService()
```

#### Step 3: ì¸ì¦ API ì—”ë“œí¬ì¸íŠ¸

**íŒŒì¼: `backend/app/api/auth.py`**
```python
from fastapi import APIRouter, HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials, OAuth2PasswordRequestForm
from app.services.auth_service import auth_service, Token, UserInDB
from pydantic import BaseModel
from datetime import timedelta

router = APIRouter()
security = HTTPBearer()

class UserCreate(BaseModel):
    username: str
    email: str
    password: str

class UserResponse(BaseModel):
    username: str
    email: str
    role: str
    is_active: bool

@router.post("/login", response_model=Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    """ë¡œê·¸ì¸ ë° í† í° ë°œê¸‰"""
    user = auth_service.authenticate_user(form_data.username, form_data.password)
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token_expires = timedelta(minutes=30)
    access_token = auth_service.create_access_token(
        data={"sub": user.username, "role": user.role},
        expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": 1800  # 30ë¶„
    }

@router.post("/register", response_model=UserResponse)
async def register_user(user_data: UserCreate):
    """ì‚¬ìš©ì ë“±ë¡"""
    try:
        user = auth_service.create_user(
            username=user_data.username,
            email=user_data.email,
            password=user_data.password
        )
        
        return UserResponse(
            username=user.username,
            email=user.email,
            role=user.role,
            is_active=user.is_active
        )
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.get("/me", response_model=UserResponse)
async def read_users_me(current_user: dict = Depends(get_current_user)):
    """í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
    return UserResponse(**current_user)

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """í˜„ì¬ ì‚¬ìš©ì ê°€ì ¸ì˜¤ê¸° (ì˜ì¡´ì„±)"""
    token = credentials.credentials
    user_data = auth_service.verify_token(token)
    
    if user_data is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ìœ íš¨í•˜ì§€ ì•Šì€ í† í°ì…ë‹ˆë‹¤",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    return user_data

async def get_admin_user(current_user: dict = Depends(get_current_user)):
    """ê´€ë¦¬ì ê¶Œí•œ í™•ì¸"""
    if current_user.get("role") != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤"
        )
    return current_user

# ë©”ì¸ ì•±ì— ë¼ìš°í„° ì¶”ê°€
# backend/app/main.pyì— ì¶”ê°€:
# from app.api.auth import router as auth_router, get_current_user, get_admin_user
# app.include_router(auth_router, prefix="/api/v1/auth", tags=["authentication"])
```

---

## ğŸ“‹ ì„±í˜¸ë‹˜ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë‹¨ê³„ 1: ê¸°ë³¸ í™˜ê²½ ì„¤ì • âœ…

```bash
# 1. ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
mkdir -p backend/{app/{api,core,models,services},training,data}
mkdir -p streamlit_app

# 2. ê°€ìƒí™˜ê²½ ì„¤ì •
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install fastapi uvicorn streamlit python-dotenv
```

### ë‹¨ê³„ 2: FastAPI ë°±ì—”ë“œ êµ¬ì¶• ğŸ”§

- [ ] **main.py ì‘ì„±** - ê¸°ë³¸ FastAPI ì„œë²„
- [ ] **ì„œë²„ ì‹¤í–‰ í…ŒìŠ¤íŠ¸** - `uvicorn app.main:app --reload`
- [ ] **CORS ì„¤ì • í™•ì¸** - í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ í…ŒìŠ¤íŠ¸
- [ ] **API ë¬¸ì„œ í™•ì¸** - `http://localhost:8000/docs`

### ë‹¨ê³„ 3: LLM API í†µí•© ğŸ§ 

- [ ] **API í‚¤ ì„¤ì •** - `.env` íŒŒì¼ ìƒì„±
- [ ] **OpenAI/Claude API ì—°ë™** - llm_service.py êµ¬í˜„
- [ ] **ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„** - /api/v1/chat
- [ ] **Streamlit ì±„íŒ… UI** - chat_interface.py

### ë‹¨ê³„ 4: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ğŸ“š

- [ ] **Chroma ì„¤ì¹˜ ë° ì„¤ì •** - `pip install chromadb`
- [ ] **ë²¡í„° ì„œë¹„ìŠ¤ êµ¬í˜„** - vector_service.py
- [ ] **ë¬¸ì„œ ì—…ë¡œë“œ API** - /api/v1/vector/upload-patents
- [ ] **ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸** - /api/v1/vector/search

### ë‹¨ê³„ 5: PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ğŸ“„

- [ ] **PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜** - PyPDF2, pdfplumber
- [ ] **ë¬¸ì„œ ì²˜ë¦¬ê¸° êµ¬í˜„** - document_processor.py
- [ ] **íŠ¹í—ˆ PDF ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸** - ì‹¤ì œ PDF íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
- [ ] **í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹ í™•ì¸**

### ë‹¨ê³„ 6: ëª¨ë¸ íŒŒì¸íŠœë‹ (ê³ ê¸‰) ğŸ¯

- [ ] **GPU í™˜ê²½ í™•ì¸** - `nvidia-smi`
- [ ] **í›ˆë ¨ ë°ì´í„° ì¤€ë¹„** - data_preparation.py
- [ ] **QLoRA ì„¤ì •** - finetune_llama.py
- [ ] **í›ˆë ¨ ì‹¤í–‰** - ì¥ì‹œê°„ ì†Œìš” (ìˆ˜ì‹œê°„~ìˆ˜ì¼)

### ë‹¨ê³„ 7: ì¸ì¦ ì‹œìŠ¤í…œ ğŸ”

- [ ] **JWT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜** - python-jose, passlib
- [ ] **ì¸ì¦ ì„œë¹„ìŠ¤ êµ¬í˜„** - auth_service.py
- [ ] **ë¡œê·¸ì¸/íšŒì›ê°€ì… API** - /api/v1/auth/login
- [ ] **í† í° ê¸°ë°˜ ë³´ì•ˆ ì ìš©**

---

## ğŸš€ ìš°ì„ ìˆœìœ„ ê°€ì´ë“œ

### ğŸ¥‡ 1ìˆœìœ„ (ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥)
1. **FastAPI ë°±ì—”ë“œ êµ¬ì¶•** - 30ë¶„~1ì‹œê°„
2. **Streamlit ì±„íŒ… UI** - 1~2ì‹œê°„
3. **ê¸°ë³¸ ì¸ì¦ ì‹œìŠ¤í…œ** - 2~3ì‹œê°„

### ğŸ¥ˆ 2ìˆœìœ„ (API í‚¤ í•„ìš”)
4. **LLM API í†µí•©** - OpenAI/Claude API í‚¤ í•„ìš”
5. **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤** - 1~2ì¼
6. **PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸** - 2~3ì¼

### ğŸ¥‰ 3ìˆœìœ„ (ê³ ê¸‰ ê¸°ëŠ¥)
7. **ëª¨ë¸ íŒŒì¸íŠœë‹** - GPU í™˜ê²½ + 1~2ì£¼
8. **ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥** - ì¶”ê°€ 1ì£¼

---

## ğŸ’¡ ì„±ê³µì„ ìœ„í•œ íŒ

### ğŸ¯ ë‹¨ê³„ë³„ ì ‘ê·¼
1. **ì‘ì€ ê²ƒë¶€í„° ì‹œì‘**: FastAPI "Hello World"ë¶€í„°
2. **ì ì§„ì  í™•ì¥**: í•œ ë²ˆì— í•˜ë‚˜ì˜ ê¸°ëŠ¥ì”© ì¶”ê°€
3. **í…ŒìŠ¤íŠ¸ ìš°ì„ **: ê° ë‹¨ê³„ë§ˆë‹¤ ë™ì‘ í™•ì¸

### ğŸ”§ ê°œë°œ í™˜ê²½ ìµœì í™”
- **VS Code í™•ì¥**: Python, FastAPI, REST Client
- **í„°ë¯¸ë„ ë¶„í• **: ì„œë²„/í´ë¼ì´ì–¸íŠ¸ ë™ì‹œ ì‹¤í–‰
- **Git ë²„ì „ê´€ë¦¬**: ê° ë‹¨ê³„ë§ˆë‹¤ ì»¤ë°‹

### ğŸ“š í•™ìŠµ ë¦¬ì†ŒìŠ¤
- **FastAPI ê³µì‹ ë¬¸ì„œ**: https://fastapi.tiangolo.com/
- **Streamlit íŠœí† ë¦¬ì–¼**: https://docs.streamlit.io/
- **LangChain ê°€ì´ë“œ**: https://python.langchain.com/

---

> ## ğŸ¤ **ì„±í˜¸ë‹˜, ì´ì œ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ ë¡œë“œë§µì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!**
>
> **í˜„ì¬ ì •ì  ì‹œìŠ¤í…œ** â†’ **ë°±ì—”ë“œ API** â†’ **LLM í†µí•©** â†’ **ë²¡í„° ê²€ìƒ‰** â†’ **ì™„ì „í•œ RAG ì‹œìŠ¤í…œ**
>
> ê° ë‹¨ê³„ë§ˆë‹¤ ë§‰íˆëŠ” ë¶€ë¶„ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”. í•¨ê»˜ ì„±ê³µì ì¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë‚˜ê°€ê² ìŠµë‹ˆë‹¤! ğŸš€